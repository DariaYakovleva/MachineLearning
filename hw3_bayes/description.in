Naive Bayes

задача spam filtering
10 кросс валидаций (разбиты уже)
10 файлов
в каждой строке: ххх_спам или хх_письмо
загнали слова в словарь и дали каждому слову индекс
тогда сообщение это 2 7 5 3 1..
title:
body:

хочется получить классификацию спам/не спам
хорошее письмо попадет в спам :(((
минимизировать -- сколько хороших писем попало в спам
ожидается мера 0,72

что спрашивают?
1) почему наивный и зачем независимые признаки? 
можно воспользоваться формулой байеса где-то
P спам = P_i1 * P_i2 * .. * P_ik
возьмем ln
все ок
2) 3 способа борьбы с плохими письмами
расказать все 3
* давайте подвигаем барьеры
* давайте влезем в классиикатор, поправим что-то внутри
лекция+вики



Всем привет!

Третья лаба будет посвящена Наивному Байесовскому классификатору. Собственно в аттаче 
находится архив, в котором датасет уже разбит на 10 частей для кросс-валидации. 
Задача состоит в классификации спама. Спам сообщения содержат в своем названии spmsg, 
нормальные сообщения содержат legit. Сам текст письма состоит из двух частей: темы и тела письма. 
Все слова заменены на инты, соответствующие их индексу в некотором глобальном словаре (своего рода анонимизация). 
Соответственно от вас требуется построить наивный Байесовский классификатор и при этом:

1). Придумать, либо протестировать, что можно делать с темой и телом письма для улучшения качества работы.
2). Как учитывать (или не учитывать) слова, которые могут встретиться в обучающей выборке, 
но могут не встретится в тестовой или наоборот.
3). Как наложить дополнительные ограничения на ваш классификатор так, чтобы хорошие письма 
практически никогда не попадали в спам, но при этом, возможно, общее качество классификации несколько уменьшилось.
4). Понимать как устроен классификатор внутри и уметь отвечать на какие-никакие вопросы по теории с ним связанной.










